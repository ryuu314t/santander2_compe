{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3108ed389c051980f763a7b75970c95f4cd4d97"
   },
   "source": [
    "# Introduction\n",
    "**\"[Distilling the Knowledge in a Neural Network](http://arxiv.org/abs/1503.02531)\" was introduced by Geoffrey Hinton, Oriol Vinyals, Jeff Dean in Mar 2015. In this kernel, I would like to share some experiments to distill the knowledge from a [LGBM teacher](https://www.kaggle.com/tanreinama/lightgbm-minimize-leaves-with-gaussiannb) (LB:0.899) to a neural network. The student network has not surpassed the teacher model yet (LB:0.888). But, I hope I can make it happen before this competition ends.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "# np.random.seed(8)\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, scale\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "from keras.losses import binary_crossentropy\n",
    "import gc\n",
    "import scipy.special\n",
    "from tqdm import *\n",
    "from scipy.stats import norm, rankdata\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b6e65eca995c134ad532f7a8acb7b106575dde9"
   },
   "source": [
    "## **Load the dataset, and the prediction of 5-fold LGBM on training set**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90cce1d9df7f0a4a838bdbe1ae3e190a9ecd7147",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\n",
    "test = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\n",
    "train_knowledge = pd.read_csv('../input/santander-2019-distillation/lgbm_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c34513f569d5fa7fa79c39f995f09b555c2e2bf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "y_knowledge = train_knowledge['target']\n",
    "id_code_train = train['ID_code']\n",
    "id_code_test = test['ID_code']\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7f26e493b56fe58fdf82fe2936bff8b5c61a846"
   },
   "source": [
    "## Adding some features: the credit belong to this kernel: https://www.kaggle.com/karangautam/keras-nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac0a1aa3268892879ccb3ebc9e36e39a4612c0dd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    train['mean_'+feature] = (train[feature].mean()-train[feature])\n",
    "    train['z_'+feature] = (train[feature] - train[feature].mean())/train[feature].std(ddof=0)\n",
    "    train['sq_'+feature] = (train[feature])**2\n",
    "    train['sqrt_'+feature] = np.abs(train[feature])**(1/2)\n",
    "    train['c_'+feature] = (train[feature])**3\n",
    "    train['p4_'+feature] = (train[feature])**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7624675031843a1c065c982ca501e95a45e4ef90",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    test['mean_'+feature] = (train[feature].mean()-test[feature])\n",
    "    test['z_'+feature] = (test[feature] - train[feature].mean())/train[feature].std(ddof=0)\n",
    "    test['sq_'+feature] = (test[feature])**2\n",
    "    test['sqrt_'+feature] = np.abs(test[feature])**(1/2)\n",
    "    test['c_'+feature] = (test[feature])**3\n",
    "    test['p4_'+feature] = (test[feature])**4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a98a587502fef9575b74f94dd7e4f41188ab4b37"
   },
   "source": [
    "## Normalize and split data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b631bca7d0fcacc45ca7f35e14717619843e46c5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussRankScaler():\n",
    "\n",
    "    def __init__( self ):\n",
    "        self.epsilon = 1e-9\n",
    "        self.lower = -1 + self.epsilon\n",
    "        self.upper =  1 - self.epsilon\n",
    "        self.range = self.upper - self.lower\n",
    "\n",
    "    def fit_transform( self, X ):\n",
    "\n",
    "        i = np.argsort( X, axis = 0 )\n",
    "        j = np.argsort( i, axis = 0 )\n",
    "\n",
    "        assert ( j.min() == 0 ).all()\n",
    "        assert ( j.max() == len( j ) - 1 ).all()\n",
    "\n",
    "        j_range = len( j ) - 1\n",
    "        self.divider = j_range / self.range\n",
    "\n",
    "        transformed = j / self.divider\n",
    "        transformed = transformed - self.upper\n",
    "        transformed = scipy.special.erfinv( transformed )\n",
    "        ############\n",
    "        # transformed = transformed - np.mean(transformed)\n",
    "\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "108750758b3cd9eeec6fd646585b73af93b162ec",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPLIT = len(train)\n",
    "train = train.append(test)\n",
    "del test; gc.collect()\n",
    "# print(train.shape)\n",
    "scaler = GaussRankScaler()\n",
    "sc = StandardScaler()\n",
    "for feat in tqdm(features):\n",
    "    train[feat] = scaler.fit_transform(train[feat])\n",
    "    train[feat+'_r'] = rankdata(train[feat]).astype('float32')\n",
    "    train[feat+'_n'] = norm.cdf(train[feat]).astype('float32')\n",
    "\n",
    "feats = [c for c in train.columns if c not in (['ID_code', 'target'] + features)]\n",
    "for feat in tqdm(feats):\n",
    "    train[feat] = sc.fit_transform(train[feat].values.reshape(-1, 1))\n",
    "\n",
    "train = train.drop(['target', 'ID_code'], axis=1)\n",
    "test = train[SPLIT:].values\n",
    "train = train[:SPLIT].values\n",
    "# test = test.drop(['ID_code'], axis=1)\n",
    "print('Done!!')\n",
    "print(train.shape)\n",
    "# train.head()\n",
    "# train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ca31e41aa686241ce55d08f44d6c2a58c029810",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "# train = scaler.fit_transform(train)\n",
    "# test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f3220d0506c8e89ce8a99d1cc8fe77e83275a17",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid  = train_test_split(train, y, y_knowledge, stratify=y, test_size=0.2, random_state=8)\n",
    "# x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid  = train_test_split(train[:SPLIT], y, y_knowledge, stratify=y, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51a4ef6c1d48eb4d83f18248256bf2884f02504c"
   },
   "source": [
    "## Define our student network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09e980734152207ef1a7cec18d717733e40edfba",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function = 'relu'\n",
    "# function = keras.layers.advanced_activations.LeakyReLU(alpha=.001)\n",
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    x = Dense(128, activation=function)(input_tensor)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(96, activation=function)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation=function)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out_put = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, out_put)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4b5cc0565c0c1ab30cf2958e98468e4493f0189"
   },
   "source": [
    "## Some necessary functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cd85490ab33a4e8583427955fc29a1de218132b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8ad256648019aa6908bdda89245d24b2865564a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 2.0\n",
    "alpha=.25\n",
    "epsilon = K.epsilon()\n",
    "\n",
    "def focal_loss(y_true, y_pred):\n",
    "    pt_1 = y_pred * y_true\n",
    "    pt_1 = K.clip(pt_1, epsilon, 1-epsilon)\n",
    "    CE_1 = -K.log(pt_1)\n",
    "    FL_1 = alpha* K.pow(1-pt_1, gamma) * CE_1\n",
    "    \n",
    "    pt_0 = (1-y_pred) * (1-y_true)\n",
    "    pt_0 = K.clip(pt_0, epsilon, 1-epsilon)\n",
    "    CE_0 = -K.log(pt_0)\n",
    "    FL_0 = (1-alpha)* K.pow(1-pt_0, gamma) * CE_0\n",
    "    \n",
    "    loss = K.sum(FL_1, axis=1) + K.sum(FL_0, axis=1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51906f2d2a1ce3d937df6d401c831cd58a7f9b4e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    # y = np.array(y)\n",
    "    # print(y)\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    sample_size = x.shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    np.random.shuffle(index_array)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index_array]\n",
    "    mixed_y = (lam * y) + ((1 - lam) * y[index_array])\n",
    "    # print((1 - lam) * y[index_array])\n",
    "    # print((lam * y).shape,((1 - lam) * y[index_array]).shape)\n",
    "    return mixed_x, mixed_y\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n",
    "\n",
    "\n",
    "def batch_generator(X,y,batch_size=128,shuffle=True,mixup=False):\n",
    "    y = np.array(y)\n",
    "    # print(X.shape[0], y.shape[0])\n",
    "    sample_size = X.shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    \n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index_array)\n",
    "        batches = make_batches(sample_size, batch_size)\n",
    "        for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "            batch_ids = index_array[batch_start:batch_end]\n",
    "            X_batch = X[batch_ids]\n",
    "            y_batch = y[batch_ids]\n",
    "            \n",
    "            if mixup:\n",
    "                # print('before', X_batch.shape, y_batch.shape)\n",
    "                X_batch,y_batch = mixup_data(X_batch,y_batch,alpha=1.0)\n",
    "            # print('*****************')    \n",
    "            yield X_batch,y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "703fe0fa610133323f862f412c8e22c3beaf0ad2"
   },
   "source": [
    "## Experiment 1\n",
    "Firstly, we check the performance of simple feed forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c657dc6f052fb182cc46027283ae2929dd7a014",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model((train.shape[1],), 1)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "checkpoint = ModelCheckpoint('feed_forward_model.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='min', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=9)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "tr_gen = batch_generator(x_train,y_train,batch_size=BATCH_SIZE,shuffle=True,mixup=True)\n",
    "\n",
    "history = model.fit_generator(# x_train,y_train,\n",
    "                                tr_gen,\n",
    "                                steps_per_epoch=np.ceil(float(len(x_train)) / float(BATCH_SIZE)),\n",
    "                                epochs=100,\n",
    "                                # batch_size = 1024,\n",
    "                                callbacks=callbacks_list,\n",
    "                                validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d024753128c64a5d31a6d861bcb1d7f38c65366",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('feed_forward_model.h5')\n",
    "prediction = model.predict(x_valid, batch_size=512, verbose=1)\n",
    "roc_auc_score(y_valid, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc3a1cf3d0df24df14f636f863f1bc67aac7659f"
   },
   "source": [
    "## Knowledge distillation\n",
    "The basic idea is that you feed both groundtruth and the prediction from the teacher model to the student network.\n",
    "Soft targets (the prediction of the teacher model) contains more information than the hard labels (groundtruth) due to the fact that they encode similarity measures between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4bc6654afc5e5a834efc5ca09a44605eb62b6bee",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.vstack((y_train, y_knowledge_train)).T\n",
    "y_valid = np.vstack((y_valid, y_knowledge_valid)).T\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a70bf72aca056687f154690347a66bba3f8026d2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knowledge_distillation_loss_withBE(y_true, y_pred, beta=0.1):\n",
    "\n",
    "    # Extract the groundtruth from dataset and the prediction from teacher model\n",
    "    y_true, y_pred_teacher = y_true[: , :1], y_true[: , 1:]\n",
    "    \n",
    "    # Extract the prediction from student model\n",
    "    y_pred, y_pred_stu = y_pred[: , :1], y_pred[: , 1:]\n",
    "\n",
    "    loss = beta*binary_crossentropy(y_true,y_pred) + (1-beta)*binary_crossentropy(y_pred_teacher, y_pred_stu)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6452176b0ffc214ea09c24c8b5a83a2ffd102c51",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_2(y_true, y_pred):\n",
    "    y_true = y_true[:, :1]\n",
    "    y_pred = y_pred[:, :1]\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "def auc_3(y_true, y_pred):\n",
    "    y_true = y_true[:, :1]\n",
    "    y_pred = y_pred[:, 1:]\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be939049ab91a59440245cc4243561fd60e4bc4c"
   },
   "source": [
    "## Experment 2\n",
    "We set the ratio between teacher's prediction and groundtruth is 1:9, and use the basic binary crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3b91e105d49537cae2d1e43294ffa4483e4298f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model((train[:SPLIT].shape[1],), 2)\n",
    "model.compile(loss=knowledge_distillation_loss_withBE, optimizer='adam', metrics=[auc_2])\n",
    "\n",
    "checkpoint = ModelCheckpoint('student_model_BE.h5', monitor='val_auc_2', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_auc_2\", \n",
    "                      mode=\"max\", \n",
    "                      patience=9)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "\n",
    "history = model.fit(x_train,y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efa0411e6495f5ff309650a6d77ddeca65948278"
   },
   "source": [
    "## Experment 3\n",
    "We set the ratio between teacher's prediction and groundtruth is 1:9, and use the focal loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "442f656984e998fc9d425b19736936c6db73bd43",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knowledge_distillation_loss_withFL(y_true, y_pred, beta=0.1):\n",
    "\n",
    "    # Extract the groundtruth from dataset and the prediction from teacher model\n",
    "    y_true, y_pred_teacher = y_true[: , :1], y_true[: , 1:]\n",
    "    \n",
    "    # Extract the prediction from student model\n",
    "    y_pred, y_pred_stu = y_pred[: , :1], y_pred[: , 1:]\n",
    "\n",
    "    loss = beta*focal_loss(y_true,y_pred) + (1-beta)*binary_crossentropy(y_pred_teacher, y_pred_stu)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6641a36d26185a8b98fc4b7001a18814951e4101",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model((train.shape[1],), 2)\n",
    "model.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2, auc_3])\n",
    "\n",
    "checkpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_auc_2\", \n",
    "                      mode=\"max\", \n",
    "                      patience=9)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "\n",
    "history = model.fit(x_train,y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec61ffa8dc03af2c7a8c88e59bee2edb8b34d740"
   },
   "source": [
    "## Experment 4\n",
    "Tuning hyper parameter \"Temperature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3283fdd87f84de160fbcfd8002aed16a7cd866ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import logit\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "\n",
    "TEMPERATURE = 2\n",
    "\n",
    "y_knowledge_logit = logit(y_knowledge)\n",
    "y_temperature = sigmoid(y_knowledge_logit/TEMPERATURE)\n",
    "\n",
    "# del x_train, x_valid; gc.collect()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid  = train_test_split(train, y, y_temperature,\n",
    "                                                                                             stratify=y, test_size=0.2, random_state=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac8842dc0d6dacb8c062cf7cfa3edcd4799e4a34",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.vstack((y_train, y_knowledge_train)).T\n",
    "y_valid = np.vstack((y_valid, y_knowledge_valid)).T\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ecf279fbb4693b6e6e2d55d1d048e520c858f22",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model((train.shape[1],), 2)\n",
    "model.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2,auc_3])\n",
    "\n",
    "checkpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_auc_2\", \n",
    "                      mode=\"max\", \n",
    "                      patience=9)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "\n",
    "history = model.fit(x_train,y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size = 1024,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ad7d4d62ed9f722334fbff7ccb6a513d15b98d1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run k-fold\n",
    "num_fold = 5\n",
    "folds = list(StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=8).split(train, y))\n",
    "# del x_train, x_valid; gc.collect()\n",
    "\n",
    "y_test_pred_log = np.zeros(len(train))\n",
    "y_train_pred_log = np.zeros(len(train))\n",
    "print(y_test_pred_log.shape)\n",
    "print(y_train_pred_log.shape)\n",
    "score = []\n",
    "\n",
    "for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "    print('\\n===================FOLD=',j)\n",
    "    x_train, x_valid = train[train_idx], train[valid_idx]\n",
    "    y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "    y_knowledge_train, y_knowledge_valid = y_temperature[train_idx], y_temperature[valid_idx]\n",
    "    \n",
    "    y_train = np.vstack((y_train, y_knowledge_train)).T\n",
    "    y_valid = np.vstack((y_valid, y_knowledge_valid)).T\n",
    "    \n",
    "    model = create_model((train.shape[1],), 2)\n",
    "    model.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2, auc_3])\n",
    "\n",
    "    checkpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n",
    "                                 save_best_only=True, mode='max', save_weights_only = True)\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n",
    "                                       verbose=1, mode='max', epsilon=0.0001)\n",
    "    early = EarlyStopping(monitor=\"val_auc_2\", \n",
    "                          mode=\"max\", \n",
    "                          patience=9)\n",
    "    callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "    history = model.fit(x_train,y_train,\n",
    "                        epochs=120,\n",
    "                        batch_size = BATCH_SIZE,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=(x_valid, y_valid))\n",
    "    \n",
    "    model.load_weights('student_model_FL.h5')\n",
    "    prediction = model.predict(x_valid,\n",
    "                               batch_size=512,\n",
    "                               verbose=1)\n",
    "    # print(prediction.shape)\n",
    "    # prediction = np.sum(prediction, axis=1)/2\n",
    "    score.append(roc_auc_score(y_valid[:,0], prediction[:,0]))\n",
    "    prediction = model.predict(train,\n",
    "                               batch_size=512,\n",
    "                               verbose=1)\n",
    "    # y_test_pred_log += np.sum(prediction, axis=1)/2\n",
    "    y_test_pred_log += np.squeeze(prediction[:, 0])\n",
    "    \n",
    "    prediction = model.predict(train,\n",
    "                               batch_size=512,\n",
    "                               verbose=1)\n",
    "    # y_train_pred_log += np.sum(prediction, axis=1)/2\n",
    "    y_train_pred_log += np.squeeze(prediction[:, 0])\n",
    "    \n",
    "    del x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid\n",
    "    gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5751d2bbb3a9b71a71a7c21f69f7d2173568cbba",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"OOF score: \", roc_auc_score(y, y_train_pred_log/num_fold))\n",
    "print(\"average {} folds score: \".format(num_fold), np.sum(score)/num_fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2739c50d9ec082329206a058ab45c5fc9d9beca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make submission\n",
    "submit = pd.read_csv('../input/santander-customer-transaction-prediction/sample_submission.csv')\n",
    "submit['ID_code'] = id_code_test\n",
    "submit['target'] = y_test_pred_log/num_fold\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "submit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
